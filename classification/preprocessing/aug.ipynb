{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Install libraries\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install textattack\n",
    "\n",
    "\n",
    "#Import libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter, CharSwapAugmenter, CheckListAugmenter, CLAREAugmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation - NOTE : Do not use these data for validation\n",
    "\n",
    "Back translation\n",
    "EDA (Easy Data Augmentation).\n",
    "NLP Albumentation.\n",
    "NLP Aug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: ##\n",
    "If you already have back.pkl in this directory no need to perform below codes\n",
    "You can just directly load back.pkl and start working with EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACK-TRANSLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "#English to french model\n",
    "en2french = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "en2french_tkn = MarianTokenizer.from_pretrained(en2french)\n",
    "en2french_model = MarianMTModel.from_pretrained(en2french)\n",
    "\n",
    "#french to english model\n",
    "french2en = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "french2en_tkn = MarianTokenizer.from_pretrained(french2en)\n",
    "french2en_model = MarianMTModel.from_pretrained(french2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = [\"THE NEW PIECEGOODS BAZAR CO., LTD.,BOMBAY vs THE COMMISSIONER OF INCOME-TAX,BOMBAY on 26 May, 1950 \",\n",
    "          \"Equivalent citations: 1950 AIR 165, 1950 SCR 553 \",\n",
    "          \"The first model translates from English to French, which is a temporary process\", \n",
    "          \"The second model finally translates back all the temporary french text into English\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring the texts to format for the model\n",
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "    return formated_bach\n",
    "\n",
    "#performs translation\n",
    "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True),max_new_tokens=200)\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    \n",
    "    return translated_texts\n",
    "\n",
    "# translated_texts = perform_translation(original_texts, en2french_model, en2french_tkn)\n",
    "\n",
    "def perform_back_translation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, en2french_model, en2french_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, french2en_model, french2en_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return back_translated_batch\n",
    "\n",
    "def combine_texts(original_texts, back_translated_batch):\n",
    "  \n",
    "  return set(original_texts + back_translated_batch) \n",
    "\n",
    "def perform_back_translation_with_augmentation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    " # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, en2french_model, en2french_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, french2en_model, french2en_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return combine_texts(original_texts, back_translated_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "REASONING    3291\n",
      "FACTS        1458\n",
      "PRECEDENT     271\n",
      "RPC           242\n",
      "STATUTE       176\n",
      "RLC           171\n",
      "COUNSEL       128\n",
      "ARG           121\n",
      "JUDGE          64\n",
      "CITATION       60\n",
      "NAME           59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../deprecated/final.csv',sep=',',names=['label','sentence'])\n",
    "df = df.drop(0)\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "#Remove labels which is occuring more than 240 times in final.csv\n",
    "df = df.drop(df[df['label'] == 'RPC'].index)\n",
    "df = df.drop(df[df['label'] == 'PRECEDENT'].index)\n",
    "df = df.drop(df[df['label'] == 'REASONING'].index)\n",
    "df = df.drop(df[df['label'] == 'FACTS'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE     176\n",
      "RLC         171\n",
      "COUNSEL     128\n",
      "ARG         121\n",
      "JUDGE        64\n",
      "CITATION     60\n",
      "NAME         59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyse the frequency of each labels\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing df sentences into a list tmp\n",
    "tmp = (df.values).tolist()\n",
    "tmp = [item[1] for item in tmp]\n",
    "\n",
    "#Storing back transalated sentences in back list\n",
    "back = []\n",
    "sz = len(tmp)\n",
    "\n",
    "#Working faster if we are sending 1 sent per loop\n",
    "for sent in tmp:\n",
    "    dum = perform_back_translation([sent])\n",
    "    back.extend(dum)\n",
    "    sz = sz-1\n",
    "    print(f\"{sz} sentences left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing back translated sentences in back.pkl as list\n",
    "\n",
    "with open('./back.pkl', 'wb') as file:\n",
    "    pickle.dump(back, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if back.pkl already exists\n",
    "\n",
    "with open('back.pkl','rb') as file:\n",
    "    back = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start each day with positive thoughts and pee your day']\n",
      "['start each day with positive thinks and make your day']\n",
      "['start each day with positive and make your day', 'thoughts each day with positive start and make your day', 'start each day apiece with positive thoughts and make your day', 'start each day with positive thoughts and give your day']\n",
      "['start each day with positive thoughts and make your dny']\n",
      "2024-04-01 00:05:15,901 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "['start each day with positive thoughts and make your day']\n"
     ]
    }
   ],
   "source": [
    "#example text\n",
    "text = \"start each day with positive thoughts and make your day\"\n",
    "\n",
    "#Create instances\n",
    "wordnet_aug = WordNetAugmenter()\n",
    "embed_aug = EmbeddingAugmenter()\n",
    "\n",
    "#RI, RS, RD, SR\n",
    "aug_type = {\"RI\":0,\"RS\":1,\"RD\":2,\"SR\":3}\n",
    "eda_aug = EasyDataAugmenter()\n",
    "\n",
    "charswap_aug = CharSwapAugmenter()\n",
    "checklist_aug = CheckListAugmenter()\n",
    "# clare_aug = CLAREAugmenter()\n",
    "\n",
    "#print augmented text\n",
    "print(wordnet_aug.augment(text))\n",
    "print(embed_aug.augment(text))\n",
    "print(eda_aug.augment(text))\n",
    "print(charswap_aug.augment(text))\n",
    "print(checklist_aug.augment(text))\n",
    "# print(clare_aug.augment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start each day with positive thoughts and establish your day']\n",
      "['initiation each day with positive thoughts and make your day']\n",
      "<class 'str'>\n",
      "['start each day with positive thoughts and make your ady']\n",
      "['start each day with positive thoughts and make your day']\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_aug.augment(text))\n",
    "print(embed_aug.augment(text))\n",
    "print(eda_aug.augment(text)[aug_type[\"SR\"]])\n",
    "print(charswap_aug.augment(text))\n",
    "print(checklist_aug.augment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations left : 0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "aug = []\n",
    "it = len(back)\n",
    "for text in back:\n",
    "\n",
    "    #Synonym replacement\n",
    "    newtxt = wordnet_aug.augment(text)[0]\n",
    "    newtxt = embed_aug.augment(newtxt)[0]\n",
    "\n",
    "    #Random Insertion\n",
    "    try:\n",
    "        newtxt = eda_aug.augment(newtxt)[aug_type[\"RI\"]]\n",
    "    except:\n",
    "        print(text)\n",
    "        print(eda_aug.augment(newtxt))\n",
    "\n",
    "    #Random Swapping\n",
    "    try:\n",
    "        newtxt = eda_aug.augment(newtxt)[aug_type[\"RS\"]]\n",
    "    except:\n",
    "        print(text)\n",
    "        print(eda_aug.augment(newtxt))\n",
    "\n",
    "    #Random Deletion\n",
    "    try:\n",
    "        newtxt = eda_aug.augment(newtxt)[aug_type[\"RD\"]]\n",
    "    except:\n",
    "        print(text)\n",
    "        print(eda_aug.augment(newtxt))\n",
    "\n",
    "    #Push newtxt in new list\n",
    "    aug.append(newtxt)\n",
    "    it -= 1\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Iterations left : {it}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing augmented sentences in aug.pkl as list\n",
    "\n",
    "with open('./aug.pkl', 'wb') as file:\n",
    "    pickle.dump(aug, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the dataset(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check compare df if you wanna see how different are new sentences than before\n",
    "\n",
    "compare = pd.DataFrame(list(zip(tmp, aug)),\n",
    "              columns=['Orig Sentences','New Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if aug.pkl already exists\n",
    "\n",
    "with open('aug.pkl','rb') as file:\n",
    "    aug = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [item[0] for item in (df.values).tolist()]\n",
    "merged_list = list(zip(labels, aug))\n",
    "extradf = pd.DataFrame(merged_list, columns=['label', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE     176\n",
      "RLC         171\n",
      "COUNSEL     128\n",
      "ARG         121\n",
      "JUDGE        64\n",
      "CITATION     60\n",
      "NAME         59\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSTATUTE     176\\nRLC         171\\nCOUNSEL     128\\nARG         121\\nJUDGE        64\\nCITATION     60\\nNAME         59\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = extradf['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "#Initial data\n",
    "'''\n",
    "STATUTE     176\n",
    "RLC         171\n",
    "COUNSEL     128\n",
    "ARG         121\n",
    "JUDGE        64\n",
    "CITATION     60\n",
    "NAME         59\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing some labels to 240\n",
    "\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'STATUTE']\n",
    "rows_to_keep = 64\n",
    "selected_rows1 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'RLC']\n",
    "rows_to_keep = 69\n",
    "selected_rows2 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'COUNSEL']\n",
    "rows_to_keep = 112\n",
    "selected_rows3 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'ARG']\n",
    "rows_to_keep = 119\n",
    "selected_rows4 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "#These are the rows index to keep and not delete\n",
    "selected = (selected_rows1.index).to_list() + (selected_rows2.index).to_list() + (selected_rows3.index).to_list() + (selected_rows4.index).to_list() + (extradf[extradf['label'] == 'JUDGE'].index).to_list() + (extradf[extradf['label'] == 'CITATION'].index).to_list() + (extradf[extradf['label'] == 'NAME'].index).to_list()\n",
    "\n",
    "selected_df = extradf.iloc[selected].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from CSV\n",
    "newdf = pd.read_csv('../deprecated/final.csv', sep=',', names=['label', 'sentence'])\n",
    "newdf = newdf.drop(0)\n",
    "\n",
    "\n",
    "#Brining labels with more than 240 counts to 240 freq\n",
    "rows_to_delete1 = newdf[newdf['label'] == 'RPC']\n",
    "selected_rows1 = rows_to_delete1.sample(n=2, random_state=42)\n",
    "newdf = newdf.drop(selected_rows1.index)\n",
    "\n",
    "rows_to_delete2 = newdf[newdf['label'] == 'PRECEDENT']\n",
    "selected_rows2 = rows_to_delete2.sample(n=31, random_state=42)\n",
    "newdf = newdf.drop(selected_rows2.index)\n",
    "\n",
    "rows_to_delete3 = newdf[newdf['label'] == 'REASONING']\n",
    "selected_rows3 = rows_to_delete3.sample(n=3051, random_state=42)\n",
    "newdf = newdf.drop(selected_rows3.index)\n",
    "\n",
    "rows_to_delete4 = newdf[newdf['label'] == 'FACTS']\n",
    "selected_rows4 = rows_to_delete4.sample(n=1218, random_state=42)\n",
    "newdf = newdf.drop(selected_rows4.index)\n",
    "\n",
    "newdf = newdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([newdf, selected_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE      240\n",
      "COUNSEL      240\n",
      "RLC          240\n",
      "FACTS        240\n",
      "REASONING    240\n",
      "RPC          240\n",
      "PRECEDENT    240\n",
      "ARG          240\n",
      "JUDGE        128\n",
      "CITATION     120\n",
      "NAME         118\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = final_df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing augmented dataset(Not preprocessed) in final_df.pkl as list\n",
    "\n",
    "with open('./final_df.pkl', 'wb') as file:\n",
    "    pickle.dump(final_df, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pp import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('final_df.pkl','rb') as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading mappings that we got from ./analysis/analysis.\n",
    "# here is our abbreviations mapping dictionary\n",
    "with open('../../intermediate/mappings.pickle','rb') as file:\n",
    "    mappings = pickle.load(file)\n",
    "\n",
    "#got these legal stopwords by analysis\n",
    "with open('../../intermediate/legal_stopwords.pickle','rb') as file:\n",
    "    legal_stopwords = pickle.load(file)\n",
    "\n",
    "texts =  (df['sentence']).to_list()\n",
    "texts = preprocess(texts,legal_stopwords,mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =  (df['label']).to_list()\n",
    "merged_list = list(zip(labels, texts))\n",
    "pp_df = pd.DataFrame(merged_list, columns=['label', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing augmented dataset(preprocessed) in final_pp_df.pkl\n",
    "\n",
    "with open('./final_pp_df.pkl', 'wb') as file:\n",
    "    pickle.dump(pp_df, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if back.pkl already exists\n",
    "\n",
    "with open('final_pp_df.pkl','rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at law-ai/InLegalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences):\n",
    "    # Tokenize the sentences\n",
    "    encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:,0,:]  # Average pooling\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvect = get_sentence_embeddings((pp_df['sentence']).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./intermediate/textvect.pickle', 'wb') as file:\n",
    "    pickle.dump(textvect, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
