{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../intermediate/x.pickle','rb') as file:\n",
    "    x = pickle.load(file)\n",
    "with open('../intermediate/y.pickle','rb') as file:\n",
    "    y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Python example\n",
    "import sys\n",
    "sys.path.append('../model/')  # Add the path to the 'lib' directory\n",
    "\n",
    "import pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at law-ai/InLegalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences):\n",
    "    # Tokenize the sentences\n",
    "    encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "        sentence_embeddings = outputs.last_hidden_state[:,0,:]  # Average pooling\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "jgsvect = []\n",
    "sumvect = []\n",
    "similarity = []\n",
    "i = 0\n",
    "csvFolderPath = '../labelled_data/'\n",
    "for filename in os.listdir(csvFolderPath):\n",
    "    #access x[i] and sum i+1.txt\n",
    "\n",
    "    #x[i]\n",
    "    jv  = get_sentence_embeddings(x[i])\n",
    "    i+=1\n",
    "    jgsvect.append(jv)\n",
    "\n",
    "    sumfile = filename[:-4] + '.txt'\n",
    "    sumFolderpath = '../dataset/train-data/summary/'\n",
    "    sumpath = os.path.join(sumFolderpath,sumfile)\n",
    "\n",
    "    temp = []\n",
    "    with open(sumpath,'r') as file:\n",
    "        for line in file:\n",
    "            temp.append(line.strip())\n",
    "    sumlst = pp.preprocess(temp)\n",
    "    sv = get_sentence_embeddings(sumlst)\n",
    "    sumvect.append(sv)\n",
    "\n",
    "    #calculate similarityfor each sentences in x[i] with its summary\n",
    "    temp = []\n",
    "    for jgsline in jv:\n",
    "        simi = 0\n",
    "        for sumline in sv:\n",
    "            simi += (torch.nn.functional.cosine_similarity(sumline, jgsline, dim=0)).item()\n",
    "        temp.append(simi)\n",
    "    similarity.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,no_of_files):\n",
    "#     dict = {1 : [], 2 : [], 3 : [], 4 : [], 5 : [], 6 : [], 7 : [], 8 : [], 9 : [], 10 : [], 11 : [], 12 : []}\n",
    "#  #for each sentences in x[i]\n",
    "#     for j in range(len(x[i])):\n",
    "#         #x[i][j] is the jth sentence in ith judgement\n",
    "#         # y[i][j] is a list containing labels\n",
    "#         for label in y[i][j]:\n",
    "#             if label!=0:\n",
    "#                 dict[label].append([similarity[i][j],j])\n",
    "    \n",
    "#     #stored values in dict\n",
    "#     #now for each dict[key] i will take last some k values and map rest other with 0\n",
    "#     for key in dict:\n",
    "#         lst = dict[key]\n",
    "#         sorted_list = sorted(lst, key=lambda x: x[0], reverse=True)\n",
    "#         sz = len(sorted_list)\n",
    "#         if sz<=1:\n",
    "#             continue\n",
    "#         else:\n",
    "#             #take first len/2 elements only i.e from 0 to len/2-1\n",
    "#             for k in range(sz//2, sz):\n",
    "#                 y[i][sorted_list[k][1]].remove(key)\n",
    "#                 if len(y[i][sorted_list[k][1]])==0:\n",
    "#                     y[i][sorted_list[k][1]].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save data to a pickle file\n",
    "\n",
    "with open('../intermediate/jgsvect.pickle', 'wb') as file:\n",
    "    pickle.dump(jgsvect, file)\n",
    "\n",
    "with open('../intermediate/sumvect.pickle', 'wb') as file:\n",
    "    pickle.dump(sumvect, file)\n",
    "\n",
    "with open('../intermediate/similarity.pickle', 'wb') as file:\n",
    "    pickle.dump(similarity, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predjgs = '../dataset/test-data/judgement/1181.txt'\n",
    "jgs = []\n",
    "with open(predjgs,'r') as file:\n",
    "    for line in file:\n",
    "        jgs.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jgslist = pp.preprocess(jgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_sentence_embeddings(jgslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../intermediate/test.pickle', 'wb') as file:\n",
    "    pickle.dump(temp, file)\n",
    "with open('../intermediate/jgs.pickle', 'wb') as file:\n",
    "    pickle.dump(jgs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
