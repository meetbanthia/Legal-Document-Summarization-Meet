{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from nltk import wordpunct_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","lz = WordNetLemmatizer()\n","\n","import re\n","import pickle\n","import os\n","import pandas as pd"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def remove_spaces_and_periods(abbreviation):\n","    '''\n","    Cleans abbreviation\n","    Cr. P. C. -> CrPC\n","    '''\n","\n","    cleaned_string = abbreviation.replace(\" \", \"\").replace(\".\", \"\")\n","    return cleaned_string\n","\n","def merge_contiguous_single_chars(strings):\n","    '''\n","    In most of the cases single characters doesnt make any sense, there are single characters that usually represents\n","    last names or a char of a abbreviation.\n","    That why we are using this behaviour to solve the problem of abbreviation by combining single characters.\n","    In this way we can indentify our abbreviation in our text.\n","    This function just merges all those characters so that we can just look at a word later on map that with its full form.\n","    '''\n","\n","    merged_strings = []\n","    current_string = \"\"\n","\n","    for s in strings:\n","        if len(s) == 1:\n","            current_string += s\n","        else:\n","            if len(current_string)==1:\n","                merged_strings.append(current_string)\n","                current_string = \"\"\n","            elif len(current_string)>1:\n","                merged_strings.append(remove_spaces_and_periods(current_string))\n","                current_string = \"\"\n","            merged_strings.append(s)\n","\n","    if current_string:\n","        merged_strings.append(remove_spaces_and_periods(current_string))\n","\n","    return merged_strings"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#Loading mappings that we got from ./analysis/analysis.\n","\n","# here is our abbreviations mapping dictionary\n","with open('./intermediate/mappings.pickle','rb') as file:\n","    mappings = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["jgslist = []\n","sumlist = []\n","fnames = []\n","\n","'''\n","A point to note here ith judgement's file name is not i int this case as we are accessing files directly by looking at directory.\n","So 'i' here is basically a index and not a filename\n","\n","jgslist[i] represents ith judgement\n","type(jgslist[i]) == list of words where each each list is basically words present in a one sentence in the original judgement file\n","\n","Similar thing applies for sumlist too\n","\n","filename of the judgement stored by jgslist[i] if fnames[i]\n","'''\n","\n","jgsfolderPath = './annotated/'\n","sumfolderPath = './dataset/train-data/summary/'\n","\n","for filename in os.listdir(jgsfolderPath):\n","    if filename == '.DS_Store':continue\n","    fnames.append(filename)\n","    jgspath = os.path.join(jgsfolderPath, filename)\n","    sumpath = os.path.join(sumfolderPath, filename)\n","    tempj = []\n","    temps = []\n","    with open(jgspath,'r',encoding='ISO-8859-1') as file:\n","        for line in file: \n","            content = wordpunct_tokenize(line)\n","            content = merge_contiguous_single_chars(content)\n","            tempj.append(content)\n","    jgslist.append(tempj)\n","    with open(sumpath,'r',encoding='ISO-8859-1') as file:\n","        for line in file: \n","            content = wordpunct_tokenize(line)\n","            content = merge_contiguous_single_chars(content)\n","            temps.append(content)\n","    sumlist.append(temps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(len(jgslist)):\n","  newj = []\n","  for lst in jgslist[i]:\n","    j=0\n","    dummy = []\n","    while j < len(lst)-1:\n","      temp = remove_spaces_and_periods(lst[j] + lst[j+1])\n","      if temp in mappings.keys():\n","        dummy.append(mappings[temp])\n","        j+=2\n","      elif lst[j] in mappings.keys():\n","        dummy.append(mappings[lst[j]])\n","        j+=1\n","      else :\n","        dummy.append(lst[j])\n","        j+=1\n","    if j<len(lst):\n","      if lst[j] in mappings.keys():\n","        dummy.append(mappings[lst[j]])\n","      else:\n","        dummy.append(lst[j])\n","    newj.append(dummy)\n","\n","  newc = []\n","  for lst in sumlist[i]:\n","    j=0\n","    dummy = []\n","    while j < len(lst)-1:\n","      temp = remove_spaces_and_periods(lst[j] + lst[j+1])\n","      if temp in mappings.keys():\n","        dummy.append(mappings[temp])\n","        j+=2\n","      elif lst[j] in mappings.keys():\n","        dummy.append(mappings[lst[j]])\n","        j+=1\n","      else :\n","        dummy.append(lst[j])\n","        j+=1\n","    if j<len(lst):\n","      if lst[j] in mappings.keys():\n","        dummy.append(mappings[lst[j]])\n","      else:\n","        dummy.append(lst[j])\n","    newc.append(dummy)\n","  \n","  jgslist[i] = newj\n","  sumlist[i] = newc"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#got these legal stopwords by analysis\n","with open('./intermediate/legal_stopwords.pickle','rb') as file:\n","    legal_stopwords = pickle.load(file)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def ValidationOfRomanNumerals(string):\n","    return bool(re.search(r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\",string))\n","\n","def value(r):\n","    if (r == 'I'):\n","        return 1\n","    if (r == 'V'):\n","        return 5\n","    if (r == 'X'):\n","        return 10\n","    if (r == 'L'):\n","        return 50\n","    if (r == 'C'):\n","        return 100\n","    if (r == 'D'):\n","        return 500\n","    if (r == 'M'):\n","        return 1000\n","    return -1\n"," \n","def romanToDecimal(str):\n","    res = 0\n","    i = 0\n","    if not ValidationOfRomanNumerals(str):\n","        return str\n","    while (i < len(str)):\n","        s1 = value(str[i])\n","        if (i + 1 < len(str)):\n","            s2 = value(str[i + 1])\n","            if (s1 >= s2):\n","                res = res + s1\n","                i = i + 1\n","            else:\n","                res = res + s2 - s1\n","                i = i + 2\n","        else:\n","            res = res + s1\n","            i = i + 1\n","    return f\"{res}\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Cleaning of judgements data\n","\n","1. Remove all characters except a-zA-Z0-9()\n","2. Convert all words to it's lower case\n","3. Remove english as well as legal stopwords\n","'''\n","restricted_words = stopwords.words('english')+legal_stopwords\n","extras = [\"'t\",\"'ve\",\"'d\",\" \",\"\"]\n","\n","jl = []\n","sl = []\n","for i in range(len(jgslist)):\n","    corpus = []\n","    for j in range(len(jgslist[i])):\n","      review = [lz.lemmatize(romanToDecimal(word.upper()).lower()) for word in jgslist[i][j] if word.lower() not in restricted_words and word.lower() not in extras]\n","      review = \" \".join(review)\n","      review = re.sub('[^a-zA-Z0-9]',' ', review)\n","      review = (re.sub(' +', ' ', review)).strip()\n","      if len(review)>0 : corpus.append(review)\n","    jl.append(corpus)\n","\n","    corpus = []\n","    for j in range(len(sumlist[i])):\n","      review = [lz.lemmatize(romanToDecimal(word.upper()).lower()) for word in sumlist[i][j] if word.lower() not in restricted_words and word.lower() not in extras]\n","      review = \" \".join(review)\n","      review = re.sub('[^a-zA-Z0-9]',' ', review)\n","      review = (re.sub(' +', ' ', review)).strip()\n","      if len(review)>0 : corpus.append(review)\n","    sl.append(corpus)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":609,"status":"ok","timestamp":1689038569420,"user":{"displayName":"Meet Banthia","userId":"06439412768108922091"},"user_tz":420},"id":"S-KQ3HTNexJK"},"outputs":[],"source":["# Save data to a pickle file\n","with open('./intermediate/jl.pickle', 'wb') as file:\n","    pickle.dump(jl, file)\n","\n","with open('./intermediate/sl.pickle', 'wb') as file:\n","    pickle.dump(sl, file)\n","\n","#Zipped in lists_to_use.zip"]},{"cell_type":"markdown","metadata":{},"source":["Things that can be done now:\n","1. Spell checker\n","2. some words are divided into two like convi niently, so if we can find a way to concatenate these type of words to reduce OOV problem"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["for i in range(len(fnames)):\n","    with open(f'./preprocessed_data/train-data/judgement/{fnames[i]}','w') as file:\n","        for sent in jl[i]:\n","            file.write(sent)\n","            file.write(\"\\n\")\n","    with open(f'./preprocessed_data/train-data/summary/{fnames[i]}','w') as file:\n","        for sent in sl[i]:\n","            file.write(sent)\n","            file.write(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMUHyAyCVXBXMl/2xRKV5qO","provenance":[]},"kernelspec":{"display_name":"Python 3.9 (pytorch)","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
