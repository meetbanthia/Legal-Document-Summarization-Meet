{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (0.1.99)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting textattack\n",
      "  Obtaining dependency information for textattack from https://files.pythonhosted.org/packages/69/85/f7878f69021c4f6583e07e285380d88f0bf2fafcef32c91dddd4db573692/textattack-0.3.9-py3-none-any.whl.metadata\n",
      "  Downloading textattack-0.3.9-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting bert-score>=0.3.5 (from textattack)\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m804.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting editdistance (from textattack)\n",
      "  Downloading editdistance-0.6.2-cp39-cp39-macosx_11_0_arm64.whl (20 kB)\n",
      "Collecting flair (from textattack)\n",
      "  Obtaining dependency information for flair from https://files.pythonhosted.org/packages/af/16/536683088c7306bc51cc3cc58605759ebd83b3f7ffd05a9399f4b99c8614/flair-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading flair-0.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (3.12.2)\n",
      "Collecting language-tool-python (from textattack)\n",
      "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
      "Collecting lemminflect (from textattack)\n",
      "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m952.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lru-dict (from textattack)\n",
      "  Obtaining dependency information for lru-dict from https://files.pythonhosted.org/packages/72/31/5252dcf464f6f39abfc3f1b38dd0e42e6ea2b41c0b867227ee13568b32f7/lru_dict-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading lru_dict-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.5 kB)\n",
      "Collecting datasets>=2.4.0 (from textattack)\n",
      "  Obtaining dependency information for datasets>=2.4.0 from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: nltk in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (2.0.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (1.11.1)\n",
      "Requirement already satisfied: torch!=1.8,>=1.7.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (2.1.0.dev20230717)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (4.30.2)\n",
      "Collecting terminaltables (from textattack)\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (4.65.0)\n",
      "Collecting word2number (from textattack)\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting num2words (from textattack)\n",
      "  Obtaining dependency information for num2words from https://files.pythonhosted.org/packages/8f/f0/ca1228af2bcbce2fdf2b23d58643c84253b88a3c1cd9dba391ca683c4b21/num2words-0.5.13-py3-none-any.whl.metadata\n",
      "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting more-itertools (from textattack)\n",
      "  Obtaining dependency information for more-itertools from https://files.pythonhosted.org/packages/50/e2/8e10e465ee3987bb7c9ab69efb91d867d93959095f4807db102d07995d94/more_itertools-10.2.0-py3-none-any.whl.metadata\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from textattack) (1.7.1)\n",
      "Collecting pinyin>=0.4.0 (from textattack)\n",
      "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba (from textattack)\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m606.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting OpenHowNet (from textattack)\n",
      "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
      "Collecting pycld2 (from textattack)\n",
      "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting click<8.1.0 (from textattack)\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
      "Requirement already satisfied: matplotlib in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from bert-score>=0.3.5->textattack) (3.7.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from bert-score>=0.3.5->textattack) (23.1)\n",
      "Collecting pyarrow>=8.0.0 (from datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/6d/40/bb7d45e1edbad74758963cfd1aeb79105bd009649770fc15c94aa07ceb94/pyarrow-15.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pyarrow-15.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from datasets>=2.4.0->textattack) (0.3.7)\n",
      "Collecting xxhash (from datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/a4/24/8092b5db6d459cb9e62409edfebb0253b3760d9ab7e4c52e5493c3b026b1/xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from datasets>=2.4.0->textattack) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
      "Collecting aiohttp (from datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/69/73/bda9aba49fa741c5390368e91a9a36012041f48d542772af097c43dd0963/aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for huggingface-hub>=0.19.4 from https://files.pythonhosted.org/packages/28/03/7d3c7153113ec59cfb31e3b8ee773f5f420a0dd7d26d40442542b96675c3/huggingface_hub-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from datasets>=2.4.0->textattack) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from pandas>=1.0.1->textattack) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from pandas>=1.0.1->textattack) (2023.3)\n",
      "Requirement already satisfied: typing-extensions in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from torch!=1.8,>=1.7.0->textattack) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers>=4.30.0->textattack) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers>=4.30.0->textattack) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from transformers>=4.30.0->textattack) (0.3.1)\n",
      "Requirement already satisfied: boto3>=1.20.27 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from flair->textattack) (1.28.3)\n",
      "Collecting bpemb>=0.3.2 (from flair->textattack)\n",
      "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting conllu>=4.0 (from flair->textattack)\n",
      "  Obtaining dependency information for conllu>=4.0 from https://files.pythonhosted.org/packages/ce/3f/70a1dc5bc536755ec082b806594598a10cfffaf0de978f51d4e0e4fdfa47/conllu-4.5.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
      "  Obtaining dependency information for deprecated>=1.2.13 from https://files.pythonhosted.org/packages/20/8d/778b7d51b981a96554f29136cd59ca7880bf58094338085bcf2a979a0e6a/Deprecated-1.2.14-py2.py3-none-any.whl.metadata\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
      "  Obtaining dependency information for ftfy>=6.1.0 from https://files.pythonhosted.org/packages/91/f8/dfa32d06cfcbdb76bc46e0f5d69c537de33f4cedb1a15cd4746ab45a6a26/ftfy-6.1.3-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gdown>=4.4.0 (from flair->textattack)\n",
      "  Obtaining dependency information for gdown>=4.4.0 from https://files.pythonhosted.org/packages/68/fb/c1bb2cfbf1ad068129e3d67f3420649d38183cca7118f4fa46cfe3c3adab/gdown-5.0.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting gensim>=4.2.0 (from flair->textattack)\n",
      "  Obtaining dependency information for gensim>=4.2.0 from https://files.pythonhosted.org/packages/db/af/18b551ae8d26b8731dbe5923565fdf96502bb9aca88a37f241d510c62dc2/gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting janome>=0.4.2 (from flair->textattack)\n",
      "  Obtaining dependency information for janome>=0.4.2 from https://files.pythonhosted.org/packages/73/7d/70f4069f4bbf0fca023e82a1fbbade6f5216365d4fe259fee1950723eca5/Janome-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langdetect>=1.0.9 (from flair->textattack)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.8.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from flair->textattack) (4.9.3)\n",
      "Collecting mpld3>=0.3 (from flair->textattack)\n",
      "  Obtaining dependency information for mpld3>=0.3 from https://files.pythonhosted.org/packages/95/6a/e3691bcc47485f38b09853207c928130571821d187cf174eed5418d45e82/mpld3-0.5.10-py3-none-any.whl.metadata\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair->textattack)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from flair->textattack) (1.3.0)\n",
      "Collecting segtok>=1.5.11 (from flair->textattack)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tabulate>=0.8.10 (from flair->textattack)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
      "  Obtaining dependency information for transformer-smaller-training-vocab>=0.2.3 from https://files.pythonhosted.org/packages/1a/cf/37ccc33c7223707c92aed9d320a03fc80474ea81876b7a25096eab6fdd59/transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from flair->textattack) (1.26.15)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
      "  Obtaining dependency information for wikipedia-api>=0.5.7 from https://files.pythonhosted.org/packages/2f/3f/919727b460d88c899d110f98d1a0c415264b5d8ad8176f14ce7ad9db0e3b/Wikipedia_API-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
      "  Obtaining dependency information for semver<4.0.0,>=3.0.0 from https://files.pythonhosted.org/packages/9a/77/0cc7a8a3bc7e53d07e8f47f147b92b0960e902b8254859f4aee5c4d7866b/semver-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from nltk->textattack) (1.3.0)\n",
      "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting anytree (from OpenHowNet->textattack)\n",
      "  Obtaining dependency information for anytree from https://files.pythonhosted.org/packages/6a/fb/ff946843e6b55ae9fda84df3964d6c233cd2261dface789f5be02ab79bc5/anytree-2.12.1-py3-none-any.whl.metadata\n",
      "  Downloading anytree-2.12.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: setuptools in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from OpenHowNet->textattack) (68.0.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.3 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from boto3>=1.20.27->flair->textattack) (1.31.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from boto3>=1.20.27->flair->textattack) (0.6.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from bpemb>=0.3.2->flair->textattack) (0.1.99)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from deprecated>=1.2.13->flair->textattack) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from aiohttp->datasets>=2.4.0->textattack) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/f9/b0/c213007560d001c9908649ff4b1dd11d1ff388235e773828e19d4637f502/yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/8b/c9/a81e9af48291954a883d35686f32308238dc968043143133b8ac9e2772af/frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.4.0->textattack)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy>=6.1.0->flair->textattack)\n",
      "  Obtaining dependency information for wcwidth<0.3.0,>=0.2.12 from https://files.pythonhosted.org/packages/fd/84/fd2ba7aafacbad3c4201d395674fc6348826569da3c0937e75505ead3528/wcwidth-0.2.13-py2.py3-none-any.whl.metadata\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from gdown>=4.4.0->flair->textattack) (4.12.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from gensim>=4.2.0->flair->textattack) (6.3.0)\n",
      "Requirement already satisfied: six in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib->bert-score>=0.3.5->textattack) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->bert-score>=0.3.5->textattack) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->bert-score>=0.3.5->textattack) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from requests->bert-score>=0.3.5->textattack) (2023.5.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from scikit-learn>=1.0.2->flair->textattack) (3.2.0)\n",
      "Collecting protobuf<=3.20.3 (from transformers>=4.30.0->textattack)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m298.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score>=0.3.5->textattack) (3.16.2)\n",
      "Collecting accelerate>=0.20.2 (from transformers>=4.30.0->textattack)\n",
      "  Obtaining dependency information for accelerate>=0.20.2 from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.3.2.post1)\n",
      "Requirement already satisfied: psutil in /Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages (from accelerate>=0.20.2->transformers>=4.30.0->textattack) (5.9.5)\n",
      "Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m198.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m359.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m355.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m499.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lru_dict-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (11 kB)\n",
      "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m471.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.3/387.3 kB\u001b[0m \u001b[31m501.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m500.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gdown-5.0.0-py3-none-any.whl (16 kB)\n",
      "Downloading gensim-4.3.2-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m114.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m48.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m338.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m81.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp39-cp39-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m803.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
      "Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pinyin, jieba, pycld2, word2number, langdetect, pptree, sqlitedict\n",
      "  Building wheel for pinyin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630477 sha256=ec4a56c0d606df1000799c977a24e948a595eb3f79f9aab3b1e2e56264c0eb8a\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/91/4d/03/beb69f9530864b62f295333257151b845f5f871b9a32665b9e\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=839dd5c8434ef64380f12bb556ca907e700b32238619c6628d55a9f0883777a1\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
      "  Building wheel for pycld2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycld2: filename=pycld2-0.41-cp39-cp39-macosx_11_0_arm64.whl size=4816776 sha256=911053547e872d574832cac429b2b662290cb3231d8bec8bceb9a1413593e314\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/ed/98/60/230df7a9368a02bf62e0e2719a31615b2aa82238dea526b637\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5567 sha256=0153efcd95bfca560c1f787258fd27b069469ee5c7ac05cf11137fb4836c8d3e\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/a0/4a/5b/d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=b0260083c1ed3f07a79ab3626f52fa40b2184f20f4b1e6b68fc79b29033ba168\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "  Building wheel for pptree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=5e9b48a3044665a9b0e64054a3cd30615331bc935a121cafbe87845fff02c58e\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/52/0e/51/514e690004ea9713bc3fdb678d5e2768fcc597d0c3b6a3abd2\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=a3cf60faee5275ea9a82b9339ac9ab45ec936ece8bf2aa4266004368608d4b4b\n",
      "  Stored in directory: /Users/meetbanthia/Library/Caches/pip/wheels/f6/48/c4/942f7a1d556fddd2348cb9ac262f251873dfd8a39afec5678e\n",
      "Successfully built pinyin jieba pycld2 word2number langdetect pptree sqlitedict\n",
      "Installing collected packages: word2number, wcwidth, sqlitedict, pycld2, pptree, pinyin, jieba, janome, docopt, xxhash, terminaltables, tabulate, semver, segtok, pyarrow-hotfix, pyarrow, protobuf, num2words, multidict, more-itertools, lru-dict, lemminflect, langdetect, ftfy, frozenlist, editdistance, deprecated, conllu, click, async-timeout, anytree, yarl, wikipedia-api, OpenHowNet, language-tool-python, huggingface-hub, gensim, aiosignal, pytorch-revgrad, mpld3, gdown, bpemb, aiohttp, accelerate, bert-score, transformer-smaller-training-vocab, datasets, flair, textattack\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.2\n",
      "    Uninstalling protobuf-4.24.2:\n",
      "      Successfully uninstalled protobuf-4.24.2\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.5\n",
      "    Uninstalling click-8.1.5:\n",
      "      Successfully uninstalled click-8.1.5\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask 2.3.2 requires click>=8.1.3, but you have click 8.0.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.26.1 aiohttp-3.9.1 aiosignal-1.3.1 anytree-2.12.1 async-timeout-4.0.3 bert-score-0.3.13 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.16.1 deprecated-1.2.14 docopt-0.6.2 editdistance-0.6.2 flair-0.13.1 frozenlist-1.4.1 ftfy-6.1.3 gdown-5.0.0 gensim-4.3.2 huggingface-hub-0.20.3 janome-0.5.0 jieba-0.42.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 more-itertools-10.2.0 mpld3-0.5.10 multidict-6.0.4 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 protobuf-3.20.3 pyarrow-15.0.0 pyarrow-hotfix-0.6 pycld2-0.41 pytorch-revgrad-0.2.0 segtok-1.5.11 semver-3.0.2 sqlitedict-2.1.0 tabulate-0.9.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 wcwidth-0.2.13 wikipedia-api-0.6.0 word2number-1.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Updating TextAttack package dependencies.\n",
      "textattack: Downloading NLTK required packages.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Install libraries\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install textattack\n",
    "\n",
    "\n",
    "#Import libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter, CharSwapAugmenter, CheckListAugmenter, CLAREAugmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meetbanthia/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "#English to french model\n",
    "en2french = 'Helsinki-NLP/opus-mt-en-fr'\n",
    "en2french_tkn = MarianTokenizer.from_pretrained(en2french)\n",
    "en2french_model = MarianMTModel.from_pretrained(en2french)\n",
    "\n",
    "#french to english model\n",
    "french2en = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "french2en_tkn = MarianTokenizer.from_pretrained(french2en)\n",
    "french2en_model = MarianMTModel.from_pretrained(french2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE NEW PIECEGOODS BAZAR CO., LTD.,BOMBAY vs THE COMMISSIONER OF INCOME-TAX,BOMBAY on 26 May, 1950 ',\n",
       " 'Equivalent citations: 1950 AIR 165, 1950 SCR 553 ',\n",
       " 'The first model translates from English to French, which is a temporary process',\n",
       " 'The second model finally translates back all the temporary french text into English']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_texts = [\"THE NEW PIECEGOODS BAZAR CO., LTD.,BOMBAY vs THE COMMISSIONER OF INCOME-TAX,BOMBAY on 26 May, 1950 \",\n",
    "          \"Equivalent citations: 1950 AIR 165, 1950 SCR 553 \",\n",
    "          \"The first model translates from English to French, which is a temporary process\", \n",
    "          \"The second model finally translates back all the temporary french text into English\"]\n",
    "\n",
    "original_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bring the texts to format for the model\n",
    "def format_batch_texts(language_code, batch_texts):\n",
    "  \n",
    "    formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n",
    "\n",
    "    return formated_bach\n",
    "\n",
    "#performs translation\n",
    "def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    formated_batch_texts = format_batch_texts(language, batch_texts)\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True),max_new_tokens=200)\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "    \n",
    "    return translated_texts\n",
    "\n",
    "# translated_texts = perform_translation(original_texts, en2french_model, en2french_tkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_back_translation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    "  # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, en2french_model, en2french_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, french2en_model, french2en_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return back_translated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtranslation with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_texts(original_texts, back_translated_batch):\n",
    "  \n",
    "  return set(original_texts + back_translated_batch) \n",
    "\n",
    "def perform_back_translation_with_augmentation(batch_texts, original_language=\"en\", temporary_language=\"fr\"):\n",
    "\n",
    " # Translate from Original to Temporary Language\n",
    "  tmp_translated_batch = perform_translation(batch_texts, en2french_model, en2french_tkn, temporary_language)\n",
    "\n",
    "  # Translate Back to English\n",
    "  back_translated_batch = perform_translation(tmp_translated_batch, french2en_model, french2en_tkn, original_language)\n",
    "\n",
    "  # Return The Final Result\n",
    "  return combine_texts(original_texts, back_translated_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final.csv',sep=',',names=['label','sentence'])\n",
    "df = df.drop(0)\n",
    "\n",
    "#Remove labels which is occuring more than 240 times in final.csv\n",
    "df = df.drop(df[df['label'] == 'RPC'].index)\n",
    "df = df.drop(df[df['label'] == 'PRECEDENT'].index)\n",
    "df = df.drop(df[df['label'] == 'REASONING'].index)\n",
    "df = df.drop(df[df['label'] == 'FACTS'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_to_delete = df[df['label'] == 'REASONING']\n",
    "# rows_to_keep = 3051\n",
    "# selected_rows = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "# df = df.drop(selected_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE     176\n",
      "RLC         171\n",
      "COUNSEL     128\n",
      "ARG         121\n",
      "JUDGE        64\n",
      "CITATION     60\n",
      "NAME         59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyse the frequency of each labels\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing df sentences into a list tmp\n",
    "tmp = (df.values).tolist()\n",
    "tmp = [item[1] for item in tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform back translation on dataframe list tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778 sentences left\n",
      "777 sentences left\n",
      "776 sentences left\n",
      "775 sentences left\n",
      "774 sentences left\n",
      "773 sentences left\n",
      "772 sentences left\n",
      "771 sentences left\n",
      "770 sentences left\n",
      "769 sentences left\n",
      "768 sentences left\n",
      "767 sentences left\n",
      "766 sentences left\n",
      "765 sentences left\n",
      "764 sentences left\n",
      "763 sentences left\n",
      "762 sentences left\n",
      "761 sentences left\n",
      "760 sentences left\n",
      "759 sentences left\n",
      "758 sentences left\n",
      "757 sentences left\n",
      "756 sentences left\n",
      "755 sentences left\n",
      "754 sentences left\n",
      "753 sentences left\n",
      "752 sentences left\n",
      "751 sentences left\n",
      "750 sentences left\n",
      "749 sentences left\n",
      "748 sentences left\n",
      "747 sentences left\n",
      "746 sentences left\n",
      "745 sentences left\n",
      "744 sentences left\n",
      "743 sentences left\n",
      "742 sentences left\n",
      "741 sentences left\n",
      "740 sentences left\n",
      "739 sentences left\n",
      "738 sentences left\n",
      "737 sentences left\n",
      "736 sentences left\n",
      "735 sentences left\n",
      "734 sentences left\n",
      "733 sentences left\n",
      "732 sentences left\n",
      "731 sentences left\n",
      "730 sentences left\n",
      "729 sentences left\n",
      "728 sentences left\n",
      "727 sentences left\n",
      "726 sentences left\n",
      "725 sentences left\n",
      "724 sentences left\n",
      "723 sentences left\n",
      "722 sentences left\n",
      "721 sentences left\n",
      "720 sentences left\n",
      "719 sentences left\n",
      "718 sentences left\n",
      "717 sentences left\n",
      "716 sentences left\n",
      "715 sentences left\n",
      "714 sentences left\n",
      "713 sentences left\n",
      "712 sentences left\n",
      "711 sentences left\n",
      "710 sentences left\n",
      "709 sentences left\n",
      "708 sentences left\n",
      "707 sentences left\n",
      "706 sentences left\n",
      "705 sentences left\n",
      "704 sentences left\n",
      "703 sentences left\n",
      "702 sentences left\n",
      "701 sentences left\n",
      "700 sentences left\n",
      "699 sentences left\n",
      "698 sentences left\n",
      "697 sentences left\n",
      "696 sentences left\n",
      "695 sentences left\n",
      "694 sentences left\n",
      "693 sentences left\n",
      "692 sentences left\n",
      "691 sentences left\n",
      "690 sentences left\n",
      "689 sentences left\n",
      "688 sentences left\n",
      "687 sentences left\n",
      "686 sentences left\n",
      "685 sentences left\n",
      "684 sentences left\n",
      "683 sentences left\n",
      "682 sentences left\n",
      "681 sentences left\n",
      "680 sentences left\n",
      "679 sentences left\n",
      "678 sentences left\n",
      "677 sentences left\n",
      "676 sentences left\n",
      "675 sentences left\n",
      "674 sentences left\n",
      "673 sentences left\n",
      "672 sentences left\n",
      "671 sentences left\n",
      "670 sentences left\n",
      "669 sentences left\n",
      "668 sentences left\n",
      "667 sentences left\n",
      "666 sentences left\n",
      "665 sentences left\n",
      "664 sentences left\n",
      "663 sentences left\n",
      "662 sentences left\n",
      "661 sentences left\n",
      "660 sentences left\n",
      "659 sentences left\n",
      "658 sentences left\n",
      "657 sentences left\n",
      "656 sentences left\n",
      "655 sentences left\n",
      "654 sentences left\n",
      "653 sentences left\n",
      "652 sentences left\n",
      "651 sentences left\n",
      "650 sentences left\n",
      "649 sentences left\n",
      "648 sentences left\n",
      "647 sentences left\n",
      "646 sentences left\n",
      "645 sentences left\n",
      "644 sentences left\n",
      "643 sentences left\n",
      "642 sentences left\n",
      "641 sentences left\n",
      "640 sentences left\n",
      "639 sentences left\n",
      "638 sentences left\n",
      "637 sentences left\n",
      "636 sentences left\n",
      "635 sentences left\n",
      "634 sentences left\n",
      "633 sentences left\n",
      "632 sentences left\n",
      "631 sentences left\n",
      "630 sentences left\n",
      "629 sentences left\n",
      "628 sentences left\n",
      "627 sentences left\n",
      "626 sentences left\n",
      "625 sentences left\n",
      "624 sentences left\n",
      "623 sentences left\n",
      "622 sentences left\n",
      "621 sentences left\n",
      "620 sentences left\n",
      "619 sentences left\n",
      "618 sentences left\n",
      "617 sentences left\n",
      "616 sentences left\n",
      "615 sentences left\n",
      "614 sentences left\n",
      "613 sentences left\n",
      "612 sentences left\n",
      "611 sentences left\n",
      "610 sentences left\n",
      "609 sentences left\n",
      "608 sentences left\n",
      "607 sentences left\n",
      "606 sentences left\n",
      "605 sentences left\n",
      "604 sentences left\n",
      "603 sentences left\n",
      "602 sentences left\n",
      "601 sentences left\n",
      "600 sentences left\n",
      "599 sentences left\n",
      "598 sentences left\n",
      "597 sentences left\n",
      "596 sentences left\n",
      "595 sentences left\n",
      "594 sentences left\n",
      "593 sentences left\n",
      "592 sentences left\n",
      "591 sentences left\n",
      "590 sentences left\n",
      "589 sentences left\n",
      "588 sentences left\n",
      "587 sentences left\n",
      "586 sentences left\n",
      "585 sentences left\n",
      "584 sentences left\n",
      "583 sentences left\n",
      "582 sentences left\n",
      "581 sentences left\n",
      "580 sentences left\n",
      "579 sentences left\n",
      "578 sentences left\n",
      "577 sentences left\n",
      "576 sentences left\n",
      "575 sentences left\n",
      "574 sentences left\n",
      "573 sentences left\n",
      "572 sentences left\n",
      "571 sentences left\n",
      "570 sentences left\n",
      "569 sentences left\n",
      "568 sentences left\n",
      "567 sentences left\n",
      "566 sentences left\n",
      "565 sentences left\n",
      "564 sentences left\n",
      "563 sentences left\n",
      "562 sentences left\n",
      "561 sentences left\n",
      "560 sentences left\n",
      "559 sentences left\n",
      "558 sentences left\n",
      "557 sentences left\n",
      "556 sentences left\n",
      "555 sentences left\n",
      "554 sentences left\n",
      "553 sentences left\n",
      "552 sentences left\n",
      "551 sentences left\n",
      "550 sentences left\n",
      "549 sentences left\n",
      "548 sentences left\n",
      "547 sentences left\n",
      "546 sentences left\n",
      "545 sentences left\n",
      "544 sentences left\n",
      "543 sentences left\n",
      "542 sentences left\n",
      "541 sentences left\n",
      "540 sentences left\n",
      "539 sentences left\n",
      "538 sentences left\n",
      "537 sentences left\n",
      "536 sentences left\n",
      "535 sentences left\n",
      "534 sentences left\n",
      "533 sentences left\n",
      "532 sentences left\n",
      "531 sentences left\n",
      "530 sentences left\n",
      "529 sentences left\n",
      "528 sentences left\n",
      "527 sentences left\n",
      "526 sentences left\n",
      "525 sentences left\n",
      "524 sentences left\n",
      "523 sentences left\n",
      "522 sentences left\n",
      "521 sentences left\n",
      "520 sentences left\n",
      "519 sentences left\n",
      "518 sentences left\n",
      "517 sentences left\n",
      "516 sentences left\n",
      "515 sentences left\n",
      "514 sentences left\n",
      "513 sentences left\n",
      "512 sentences left\n",
      "511 sentences left\n",
      "510 sentences left\n",
      "509 sentences left\n",
      "508 sentences left\n",
      "507 sentences left\n",
      "506 sentences left\n",
      "505 sentences left\n",
      "504 sentences left\n",
      "503 sentences left\n",
      "502 sentences left\n",
      "501 sentences left\n",
      "500 sentences left\n",
      "499 sentences left\n",
      "498 sentences left\n",
      "497 sentences left\n",
      "496 sentences left\n",
      "495 sentences left\n",
      "494 sentences left\n",
      "493 sentences left\n",
      "492 sentences left\n",
      "491 sentences left\n",
      "490 sentences left\n",
      "489 sentences left\n",
      "488 sentences left\n",
      "487 sentences left\n",
      "486 sentences left\n",
      "485 sentences left\n",
      "484 sentences left\n",
      "483 sentences left\n",
      "482 sentences left\n",
      "481 sentences left\n",
      "480 sentences left\n",
      "479 sentences left\n",
      "478 sentences left\n",
      "477 sentences left\n",
      "476 sentences left\n",
      "475 sentences left\n",
      "474 sentences left\n",
      "473 sentences left\n",
      "472 sentences left\n",
      "471 sentences left\n",
      "470 sentences left\n",
      "469 sentences left\n",
      "468 sentences left\n",
      "467 sentences left\n",
      "466 sentences left\n",
      "465 sentences left\n",
      "464 sentences left\n",
      "463 sentences left\n",
      "462 sentences left\n",
      "461 sentences left\n",
      "460 sentences left\n",
      "459 sentences left\n",
      "458 sentences left\n",
      "457 sentences left\n",
      "456 sentences left\n",
      "455 sentences left\n",
      "454 sentences left\n",
      "453 sentences left\n",
      "452 sentences left\n",
      "451 sentences left\n",
      "450 sentences left\n",
      "449 sentences left\n",
      "448 sentences left\n",
      "447 sentences left\n",
      "446 sentences left\n",
      "445 sentences left\n",
      "444 sentences left\n",
      "443 sentences left\n",
      "442 sentences left\n",
      "441 sentences left\n",
      "440 sentences left\n",
      "439 sentences left\n",
      "438 sentences left\n",
      "437 sentences left\n",
      "436 sentences left\n",
      "435 sentences left\n",
      "434 sentences left\n",
      "433 sentences left\n",
      "432 sentences left\n",
      "431 sentences left\n",
      "430 sentences left\n",
      "429 sentences left\n",
      "428 sentences left\n",
      "427 sentences left\n",
      "426 sentences left\n",
      "425 sentences left\n",
      "424 sentences left\n",
      "423 sentences left\n",
      "422 sentences left\n",
      "421 sentences left\n",
      "420 sentences left\n",
      "419 sentences left\n",
      "418 sentences left\n",
      "417 sentences left\n",
      "416 sentences left\n",
      "415 sentences left\n",
      "414 sentences left\n",
      "413 sentences left\n",
      "412 sentences left\n",
      "411 sentences left\n",
      "410 sentences left\n",
      "409 sentences left\n",
      "408 sentences left\n",
      "407 sentences left\n",
      "406 sentences left\n",
      "405 sentences left\n",
      "404 sentences left\n",
      "403 sentences left\n",
      "402 sentences left\n",
      "401 sentences left\n",
      "400 sentences left\n",
      "399 sentences left\n",
      "398 sentences left\n",
      "397 sentences left\n",
      "396 sentences left\n",
      "395 sentences left\n",
      "394 sentences left\n",
      "393 sentences left\n",
      "392 sentences left\n",
      "391 sentences left\n",
      "390 sentences left\n",
      "389 sentences left\n",
      "388 sentences left\n",
      "387 sentences left\n",
      "386 sentences left\n",
      "385 sentences left\n",
      "384 sentences left\n",
      "383 sentences left\n",
      "382 sentences left\n",
      "381 sentences left\n",
      "380 sentences left\n",
      "379 sentences left\n",
      "378 sentences left\n",
      "377 sentences left\n",
      "376 sentences left\n",
      "375 sentences left\n",
      "374 sentences left\n",
      "373 sentences left\n",
      "372 sentences left\n",
      "371 sentences left\n",
      "370 sentences left\n",
      "369 sentences left\n",
      "368 sentences left\n",
      "367 sentences left\n",
      "366 sentences left\n",
      "365 sentences left\n",
      "364 sentences left\n",
      "363 sentences left\n",
      "362 sentences left\n",
      "361 sentences left\n",
      "360 sentences left\n",
      "359 sentences left\n",
      "358 sentences left\n",
      "357 sentences left\n",
      "356 sentences left\n",
      "355 sentences left\n",
      "354 sentences left\n",
      "353 sentences left\n",
      "352 sentences left\n",
      "351 sentences left\n",
      "350 sentences left\n",
      "349 sentences left\n",
      "348 sentences left\n",
      "347 sentences left\n",
      "346 sentences left\n",
      "345 sentences left\n",
      "344 sentences left\n",
      "343 sentences left\n",
      "342 sentences left\n",
      "341 sentences left\n",
      "340 sentences left\n",
      "339 sentences left\n",
      "338 sentences left\n",
      "337 sentences left\n",
      "336 sentences left\n",
      "335 sentences left\n",
      "334 sentences left\n",
      "333 sentences left\n",
      "332 sentences left\n",
      "331 sentences left\n",
      "330 sentences left\n",
      "329 sentences left\n",
      "328 sentences left\n",
      "327 sentences left\n",
      "326 sentences left\n",
      "325 sentences left\n",
      "324 sentences left\n",
      "323 sentences left\n",
      "322 sentences left\n",
      "321 sentences left\n",
      "320 sentences left\n",
      "319 sentences left\n",
      "318 sentences left\n",
      "317 sentences left\n",
      "316 sentences left\n",
      "315 sentences left\n",
      "314 sentences left\n",
      "313 sentences left\n",
      "312 sentences left\n",
      "311 sentences left\n",
      "310 sentences left\n",
      "309 sentences left\n",
      "308 sentences left\n",
      "307 sentences left\n",
      "306 sentences left\n",
      "305 sentences left\n",
      "304 sentences left\n",
      "303 sentences left\n",
      "302 sentences left\n",
      "301 sentences left\n",
      "300 sentences left\n",
      "299 sentences left\n",
      "298 sentences left\n",
      "297 sentences left\n",
      "296 sentences left\n",
      "295 sentences left\n",
      "294 sentences left\n",
      "293 sentences left\n",
      "292 sentences left\n",
      "291 sentences left\n",
      "290 sentences left\n",
      "289 sentences left\n",
      "288 sentences left\n",
      "287 sentences left\n",
      "286 sentences left\n",
      "285 sentences left\n",
      "284 sentences left\n",
      "283 sentences left\n",
      "282 sentences left\n",
      "281 sentences left\n",
      "280 sentences left\n",
      "279 sentences left\n",
      "278 sentences left\n",
      "277 sentences left\n",
      "276 sentences left\n",
      "275 sentences left\n",
      "274 sentences left\n",
      "273 sentences left\n",
      "272 sentences left\n",
      "271 sentences left\n",
      "270 sentences left\n",
      "269 sentences left\n",
      "268 sentences left\n",
      "267 sentences left\n",
      "266 sentences left\n",
      "265 sentences left\n",
      "264 sentences left\n",
      "263 sentences left\n",
      "262 sentences left\n",
      "261 sentences left\n",
      "260 sentences left\n",
      "259 sentences left\n",
      "258 sentences left\n",
      "257 sentences left\n",
      "256 sentences left\n",
      "255 sentences left\n",
      "254 sentences left\n",
      "253 sentences left\n",
      "252 sentences left\n",
      "251 sentences left\n",
      "250 sentences left\n",
      "249 sentences left\n",
      "248 sentences left\n",
      "247 sentences left\n",
      "246 sentences left\n",
      "245 sentences left\n",
      "244 sentences left\n",
      "243 sentences left\n",
      "242 sentences left\n",
      "241 sentences left\n",
      "240 sentences left\n",
      "239 sentences left\n",
      "238 sentences left\n",
      "237 sentences left\n",
      "236 sentences left\n",
      "235 sentences left\n",
      "234 sentences left\n",
      "233 sentences left\n",
      "232 sentences left\n",
      "231 sentences left\n",
      "230 sentences left\n",
      "229 sentences left\n",
      "228 sentences left\n",
      "227 sentences left\n",
      "226 sentences left\n",
      "225 sentences left\n",
      "224 sentences left\n",
      "223 sentences left\n",
      "222 sentences left\n",
      "221 sentences left\n",
      "220 sentences left\n",
      "219 sentences left\n",
      "218 sentences left\n",
      "217 sentences left\n",
      "216 sentences left\n",
      "215 sentences left\n",
      "214 sentences left\n",
      "213 sentences left\n",
      "212 sentences left\n",
      "211 sentences left\n",
      "210 sentences left\n",
      "209 sentences left\n",
      "208 sentences left\n",
      "207 sentences left\n",
      "206 sentences left\n",
      "205 sentences left\n",
      "204 sentences left\n",
      "203 sentences left\n",
      "202 sentences left\n",
      "201 sentences left\n",
      "200 sentences left\n",
      "199 sentences left\n",
      "198 sentences left\n",
      "197 sentences left\n",
      "196 sentences left\n",
      "195 sentences left\n",
      "194 sentences left\n",
      "193 sentences left\n",
      "192 sentences left\n",
      "191 sentences left\n",
      "190 sentences left\n",
      "189 sentences left\n",
      "188 sentences left\n",
      "187 sentences left\n",
      "186 sentences left\n",
      "185 sentences left\n",
      "184 sentences left\n",
      "183 sentences left\n",
      "182 sentences left\n",
      "181 sentences left\n",
      "180 sentences left\n",
      "179 sentences left\n",
      "178 sentences left\n",
      "177 sentences left\n",
      "176 sentences left\n",
      "175 sentences left\n",
      "174 sentences left\n",
      "173 sentences left\n",
      "172 sentences left\n",
      "171 sentences left\n",
      "170 sentences left\n",
      "169 sentences left\n",
      "168 sentences left\n",
      "167 sentences left\n",
      "166 sentences left\n",
      "165 sentences left\n",
      "164 sentences left\n",
      "163 sentences left\n",
      "162 sentences left\n",
      "161 sentences left\n",
      "160 sentences left\n",
      "159 sentences left\n",
      "158 sentences left\n",
      "157 sentences left\n",
      "156 sentences left\n",
      "155 sentences left\n",
      "154 sentences left\n",
      "153 sentences left\n",
      "152 sentences left\n",
      "151 sentences left\n",
      "150 sentences left\n",
      "149 sentences left\n",
      "148 sentences left\n",
      "147 sentences left\n",
      "146 sentences left\n",
      "145 sentences left\n",
      "144 sentences left\n",
      "143 sentences left\n",
      "142 sentences left\n",
      "141 sentences left\n",
      "140 sentences left\n",
      "139 sentences left\n",
      "138 sentences left\n",
      "137 sentences left\n",
      "136 sentences left\n",
      "135 sentences left\n",
      "134 sentences left\n",
      "133 sentences left\n",
      "132 sentences left\n",
      "131 sentences left\n",
      "130 sentences left\n",
      "129 sentences left\n",
      "128 sentences left\n",
      "127 sentences left\n",
      "126 sentences left\n",
      "125 sentences left\n",
      "124 sentences left\n",
      "123 sentences left\n",
      "122 sentences left\n",
      "121 sentences left\n",
      "120 sentences left\n",
      "119 sentences left\n",
      "118 sentences left\n",
      "117 sentences left\n",
      "116 sentences left\n",
      "115 sentences left\n",
      "114 sentences left\n",
      "113 sentences left\n",
      "112 sentences left\n",
      "111 sentences left\n",
      "110 sentences left\n",
      "109 sentences left\n",
      "108 sentences left\n",
      "107 sentences left\n",
      "106 sentences left\n",
      "105 sentences left\n",
      "104 sentences left\n",
      "103 sentences left\n",
      "102 sentences left\n",
      "101 sentences left\n",
      "100 sentences left\n",
      "99 sentences left\n",
      "98 sentences left\n",
      "97 sentences left\n",
      "96 sentences left\n",
      "95 sentences left\n",
      "94 sentences left\n",
      "93 sentences left\n",
      "92 sentences left\n",
      "91 sentences left\n",
      "90 sentences left\n",
      "89 sentences left\n",
      "88 sentences left\n",
      "87 sentences left\n",
      "86 sentences left\n",
      "85 sentences left\n",
      "84 sentences left\n",
      "83 sentences left\n",
      "82 sentences left\n",
      "81 sentences left\n",
      "80 sentences left\n",
      "79 sentences left\n",
      "78 sentences left\n",
      "77 sentences left\n",
      "76 sentences left\n",
      "75 sentences left\n",
      "74 sentences left\n",
      "73 sentences left\n",
      "72 sentences left\n",
      "71 sentences left\n",
      "70 sentences left\n",
      "69 sentences left\n",
      "68 sentences left\n",
      "67 sentences left\n",
      "66 sentences left\n",
      "65 sentences left\n",
      "64 sentences left\n",
      "63 sentences left\n",
      "62 sentences left\n",
      "61 sentences left\n",
      "60 sentences left\n",
      "59 sentences left\n",
      "58 sentences left\n",
      "57 sentences left\n",
      "56 sentences left\n",
      "55 sentences left\n",
      "54 sentences left\n",
      "53 sentences left\n",
      "52 sentences left\n",
      "51 sentences left\n",
      "50 sentences left\n",
      "49 sentences left\n",
      "48 sentences left\n",
      "47 sentences left\n",
      "46 sentences left\n",
      "45 sentences left\n",
      "44 sentences left\n",
      "43 sentences left\n",
      "42 sentences left\n",
      "41 sentences left\n",
      "40 sentences left\n",
      "39 sentences left\n",
      "38 sentences left\n",
      "37 sentences left\n",
      "36 sentences left\n",
      "35 sentences left\n",
      "34 sentences left\n",
      "33 sentences left\n",
      "32 sentences left\n",
      "31 sentences left\n",
      "30 sentences left\n",
      "29 sentences left\n",
      "28 sentences left\n",
      "27 sentences left\n",
      "26 sentences left\n",
      "25 sentences left\n",
      "24 sentences left\n",
      "23 sentences left\n",
      "22 sentences left\n",
      "21 sentences left\n",
      "20 sentences left\n",
      "19 sentences left\n",
      "18 sentences left\n",
      "17 sentences left\n",
      "16 sentences left\n",
      "15 sentences left\n",
      "14 sentences left\n",
      "13 sentences left\n",
      "12 sentences left\n",
      "11 sentences left\n",
      "10 sentences left\n",
      "9 sentences left\n",
      "8 sentences left\n",
      "7 sentences left\n",
      "6 sentences left\n",
      "5 sentences left\n",
      "4 sentences left\n",
      "3 sentences left\n",
      "2 sentences left\n",
      "1 sentences left\n",
      "0 sentences left\n"
     ]
    }
   ],
   "source": [
    "back = []\n",
    "sz = len(tmp)\n",
    "for sent in tmp:\n",
    "    dum = perform_back_translation([sent])\n",
    "    back.extend(dum)\n",
    "    sz = sz-1\n",
    "    print(f\"{sz} sentences left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./back.pkl', 'wb') as file:\n",
    "    pickle.dump(back, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./back.pkl','rb') as file:\n",
    "    back = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
      "100%|██████████| 481M/481M [24:04<00:00, 333kB/s]     \n",
      "textattack: Unzipping file /Users/meetbanthia/.cache/textattack/tmp6o1oer9n.zip to /Users/meetbanthia/.cache/textattack/word_embeddings/paragramcf.\n",
      "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/meetbanthia/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013aefb1a2d94a8e9f58ef51004c989f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee27027fa26a4bdc8a0b918ccf09e9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba95fcb82fb4d2b9979d14ef2105b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2466a584f4624e8cb76012faad41e7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ac05c51d9d4650bbf6c1f75e4069d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['starting each day with positive thoughts and make your day']\n",
      "['launched each day with positive thoughts and make your day']\n",
      "['start each day with positive thoughts and make day your', 'start day with positive thoughts and make your day', 'start each day with positive view and make your day', 'start each day with mentation positive thoughts and make your day']\n",
      "['start each day with psitive thoughts and make your day']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2b65c9b49c4a58863a6d5a2f5ecd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/432M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-25 23:39:10,288 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "['start each day with positive thoughts and make your day']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(charswap_aug\u001b[38;5;241m.\u001b[39maugment(text))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(checklist_aug\u001b[38;5;241m.\u001b[39maugment(text))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclare_aug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/augmentation/augmenter.py:125\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    122\u001b[0m words_swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m words_swapped \u001b[38;5;241m<\u001b[39m num_words_to_swap:\n\u001b[0;32m--> 125\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transformation_constraints\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Get rid of transformations we already have\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    131\u001b[0m         t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transformed_texts \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_transformed_texts\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/transformations/composite_transformation.py:39\u001b[0m, in \u001b[0;36mCompositeTransformation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m new_attacked_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n\u001b[0;32m---> 39\u001b[0m     new_attacked_texts\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(new_attacked_texts)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/transformations/transformation.py:57\u001b[0m, in \u001b[0;36mTransformation.__call__\u001b[0;34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[0;32m---> 57\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[1;32m     59\u001b[0m     text\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_transformation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/transformations/word_merges/word_merge_masked_lm.py:162\u001b[0m, in \u001b[0;36mWordMergeMaskedLM._get_transformations\u001b[0;34m(self, current_text, indices_to_modify)\u001b[0m\n\u001b[1;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    163\u001b[0m     current_text\u001b[38;5;241m.\u001b[39mpos_of_word_index(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[1;32m    164\u001b[0m ]\n\u001b[1;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[1;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/transformations/word_merges/word_merge_masked_lm.py:163\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m indices_to_modify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_to_modify)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# find indices that are suitable to merge\u001b[39;00m\n\u001b[1;32m    162\u001b[0m token_tags \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 163\u001b[0m     \u001b[43mcurrent_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_of_word_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(current_text\u001b[38;5;241m.\u001b[39mnum_words)\n\u001b[1;32m    164\u001b[0m ]\n\u001b[1;32m    165\u001b[0m merge_indices \u001b[38;5;241m=\u001b[39m find_merge_index(token_tags)\n\u001b[1;32m    166\u001b[0m merged_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merged_words(current_text, merge_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/shared/attacked_text.py:141\u001b[0m, in \u001b[0;36mAttackedText.pos_of_word_index\u001b[0;34m(self, desired_word_idx)\u001b[0m\n\u001b[1;32m    139\u001b[0m     textattack\u001b[38;5;241m.\u001b[39mshared\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mflair_tag(sentence)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_tags \u001b[38;5;241m=\u001b[39m sentence\n\u001b[0;32m--> 141\u001b[0m flair_word_list, flair_pos_list \u001b[38;5;241m=\u001b[39m \u001b[43mtextattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip_flair_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos_tags\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    147\u001b[0m         word \u001b[38;5;129;01min\u001b[39;00m flair_word_list\n\u001b[1;32m    148\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword absent in flair returned part-of-speech tags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/textattack/shared/utils/strings.py:245\u001b[0m, in \u001b[0;36mzip_flair_result\u001b[0;34m(pred, tag_type)\u001b[0m\n\u001b[1;32m    243\u001b[0m word_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag_type:\n\u001b[0;32m--> 245\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_value)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    247\u001b[0m     pos_list\u001b[38;5;241m.\u001b[39mappend(token\u001b[38;5;241m.\u001b[39mget_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'upos'"
     ]
    }
   ],
   "source": [
    "text = \"start each day with positive thoughts and make your day\"\n",
    "\n",
    "#Create instances\n",
    "wordnet_aug = WordNetAugmenter()\n",
    "embed_aug = EmbeddingAugmenter()\n",
    "eda_aug = EasyDataAugmenter()\n",
    "charswap_aug = CharSwapAugmenter()\n",
    "checklist_aug = CheckListAugmenter()\n",
    "# clare_aug = CLAREAugmenter()\n",
    "\n",
    "#print augmented text\n",
    "print(wordnet_aug.augment(text))\n",
    "print(embed_aug.augment(text))\n",
    "print(eda_aug.augment(text))\n",
    "print(charswap_aug.augment(text))\n",
    "print(checklist_aug.augment(text))\n",
    "# print(clare_aug.augment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform synonym replacement on back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = [wordnet_aug.augment(text)[0] for text in back]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [item[0] for item in (df.values).tolist()]\n",
    "merged_list = list(zip(labels, aug))\n",
    "extradf = pd.DataFrame(merged_list, columns=['label', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE     176\n",
      "RLC         171\n",
      "COUNSEL     128\n",
      "ARG         121\n",
      "JUDGE        64\n",
      "CITATION     60\n",
      "NAME         59\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSTATUTE     176\\nRLC         171\\nCOUNSEL     128\\nARG         121\\nJUDGE        64\\nCITATION     60\\nNAME         59\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = extradf['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "#Initial data\n",
    "'''\n",
    "STATUTE     176\n",
    "RLC         171\n",
    "COUNSEL     128\n",
    "ARG         121\n",
    "JUDGE        64\n",
    "CITATION     60\n",
    "NAME         59\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_delete = extradf[extradf['label'] == 'STATUTE']\n",
    "rows_to_keep = 64\n",
    "selected_rows1 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'RLC']\n",
    "rows_to_keep = 69\n",
    "selected_rows2 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'COUNSEL']\n",
    "rows_to_keep = 112\n",
    "selected_rows3 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "rows_to_delete = extradf[extradf['label'] == 'ARG']\n",
    "rows_to_keep = 119\n",
    "selected_rows4 = rows_to_delete.sample(n=rows_to_keep, random_state=42)\n",
    "\n",
    "selected = (selected_rows1.index).to_list() + (selected_rows2.index).to_list() + (selected_rows3.index).to_list() + (selected_rows4.index).to_list() + (extradf[extradf['label'] == 'JUDGE'].index).to_list() + (extradf[extradf['label'] == 'CITATION'].index).to_list() + (extradf[extradf['label'] == 'NAME'].index).to_list()\n",
    "\n",
    "selected_df = extradf.iloc[selected].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STATUTE</td>\n",
       "      <td>(2) Where a farmer has paid a creditor twice t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATUTE</td>\n",
       "      <td>Article 5 of the Ordinance preface several ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STATUTE</td>\n",
       "      <td>It was urged that section 3(b) of the Jaipur A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STATUTE</td>\n",
       "      <td>Section 21 of the Act render that, if the Reve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STATUTE</td>\n",
       "      <td>Article 13, paragraph 1, with which we are int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>NAME</td>\n",
       "      <td>Mahant Pragdasji Guru... fivesome Patel Ishwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>NAME</td>\n",
       "      <td>Amjad Khan v. The commonwealth on 20 March 1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>NAME</td>\n",
       "      <td>Raja Bhupendra Narain Singha... quintuplet Mah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>NAME</td>\n",
       "      <td>Gnanambal Ammal vs tonne. Raju Ayyar and other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NAME</td>\n",
       "      <td>Yeswant Deorao Deshmukh fivesome Walchand Ramc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           sentence\n",
       "0    STATUTE  (2) Where a farmer has paid a creditor twice t...\n",
       "1    STATUTE  Article 5 of the Ordinance preface several ame...\n",
       "2    STATUTE  It was urged that section 3(b) of the Jaipur A...\n",
       "3    STATUTE  Section 21 of the Act render that, if the Reve...\n",
       "4    STATUTE  Article 13, paragraph 1, with which we are int...\n",
       "..       ...                                                ...\n",
       "542     NAME  Mahant Pragdasji Guru... fivesome Patel Ishwar...\n",
       "543     NAME    Amjad Khan v. The commonwealth on 20 March 1952\n",
       "544     NAME  Raja Bhupendra Narain Singha... quintuplet Mah...\n",
       "545     NAME  Gnanambal Ammal vs tonne. Raju Ayyar and other...\n",
       "546     NAME  Yeswant Deorao Deshmukh fivesome Walchand Ramc...\n",
       "\n",
       "[547 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from CSV\n",
    "newdf = pd.read_csv('final.csv', sep=',', names=['label', 'sentence'])\n",
    "newdf = newdf.drop(0)\n",
    "\n",
    "# Select rows based on conditions\n",
    "rows_to_delete1 = newdf[newdf['label'] == 'RPC']\n",
    "selected_rows1 = rows_to_delete1.sample(n=2, random_state=42)\n",
    "newdf = newdf.drop(selected_rows1.index)\n",
    "\n",
    "rows_to_delete2 = newdf[newdf['label'] == 'PRECEDENT']\n",
    "selected_rows2 = rows_to_delete2.sample(n=31, random_state=42)\n",
    "newdf = newdf.drop(selected_rows2.index)\n",
    "\n",
    "rows_to_delete3 = newdf[newdf['label'] == 'REASONING']\n",
    "selected_rows3 = rows_to_delete3.sample(n=3051, random_state=42)\n",
    "newdf = newdf.drop(selected_rows3.index)\n",
    "\n",
    "rows_to_delete4 = newdf[newdf['label'] == 'FACTS']\n",
    "selected_rows4 = rows_to_delete4.sample(n=1218, random_state=42)\n",
    "newdf = newdf.drop(selected_rows4.index)\n",
    "\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'STATUTE'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'RLC'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'COUNSEL'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'ARG'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'JUDGE'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'CITATION'].index)\n",
    "newdf = newdf.drop(newdf[newdf['label'] == 'NAME'].index)\n",
    "\n",
    "newdf = newdf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this selected_df with df and create a new df\n",
    "result_df = pd.concat([newdf, selected_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "FACTS        240\n",
      "REASONING    240\n",
      "RPC          240\n",
      "PRECEDENT    240\n",
      "ARG          119\n",
      "COUNSEL      112\n",
      "RLC           69\n",
      "STATUTE       64\n",
      "JUDGE         64\n",
      "CITATION      60\n",
      "NAME          59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = result_df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df, result_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "STATUTE      240\n",
      "COUNSEL      240\n",
      "RLC          240\n",
      "ARG          240\n",
      "FACTS        240\n",
      "REASONING    240\n",
      "RPC          240\n",
      "PRECEDENT    240\n",
      "JUDGE        128\n",
      "CITATION     120\n",
      "NAME         118\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = final_df['label'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
