{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r46hJVEdE_f_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/Users/meetbanthia/Documents/HaleLab/jgslist_latest.pickle', 'rb') as file:\n",
        "  jgslist = pickle.load(file)\n",
        "\n",
        "with open('/Users/meetbanthia/Documents/HaleLab/sumlist_new.pickle', 'rb') as file:\n",
        "  sumlist = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(jgslist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7iV2c6CG1fw",
        "outputId": "a52cd018-95c0-4b45-8def-415e9345caeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7028"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sumlist[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB-uPcEjaI6n",
        "outputId": "2293d995-a6c8-404d-9256-aae9a1ca38a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['purchased land gili teppa nanjappa pendency land acquisi tion proceedings city bangalore improvement 1945',\n",
              " 'acquisition building house colony housing board whose statutory responsibility implement housing schemesection challenged acquisition filing writ petition',\n",
              " 'dismissed writ petition',\n",
              " 'special leave contended',\n",
              " 'non compliance mandatory requirement 16 requires service notice every person whose name appears land revenue register primarily liable pay property land revenue',\n",
              " '15 3 provides main scheme provide construction buildings proper working classes including whole part classes displaced execution scheme',\n",
              " 'whose land acquired displaced entitled allotment land construction building',\n",
              " 'residence',\n",
              " 'dismissing evidence rightly revenue register names predecessors title giliteppa nanjappa shown given due notice',\n",
              " 'document produced revenue register contemplated 16',\n",
              " '167a c f 15 3 impose compulsory duty right claim plot',\n",
              " 'clear rule 10 person displaced acquisition accomodated',\n",
              " 'however benefi cient consideration necessary obligation',\n",
              " '168a c observed land available fulfils criteria prescribed rules consider claim']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "metadata": {
        "id": "PNvi-sNr0Yer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is glove embeddings\n",
        "# Spacy uses GLoVE word vectors of 300 dimensions trained on Commoncrawl corpus.\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "tokens = nlp(\"apple cat sky\")\n",
        "\n",
        "\n",
        "print(tokens.text, tokens.vector[:3], tokens.vector_norm) # Only the first three components of the vector\n",
        "\n",
        "print(type(tokens.vector))\n",
        "for token in tokens:\n",
        "    print(token.text, token.vector[:3], token.vector_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2FdVu-CaLFf",
        "outputId": "b20e753e-02a2-4066-ca5c-2f840d88c610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple cat sky [ 3.2824001  1.7343334 -4.7820168] 41.221966020414214\n",
            "<class 'numpy.ndarray'>\n",
            "apple [-1.0084  -2.0308  -0.64185] 43.366478\n",
            "cat [ 3.7032  4.1982 -5.0002] 63.188496\n",
            "sky [ 7.1524  3.0356 -8.704 ] 76.302284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# infersent\n",
        "import torch\n",
        "\n",
        "from models import InferSent\n",
        "V = 2\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "infersent = InferSent(params_model)\n",
        "infersent.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "metadata": {
        "id": "dGXvmVKdgGix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a413d7b9-ac27-4532-f73f-41188a1d69e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#infersent is our model\n",
        "\n",
        "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
        "infersent.set_w2v_path(W2V_PATH)"
      ],
      "metadata": {
        "id": "DTUd0JnrupS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I ate dinner.\",\n",
        "       \"We had a three-course meal.\",\n",
        "       \"Brad came to dinner with us.\",\n",
        "       \"He loves fish tacos.\",\n",
        "       \"In the end, we all felt like we ate too much.\",\n",
        "       \"We all agreed; it was a magnificent evening.\"]"
      ],
      "metadata": {
        "id": "FBGywJRyygbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infersent.build_vocab(sentences, tokenize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxB5hUWHwhcD",
        "outputId": "3898cb6d-3ba5-4f0c-dfb8-ba55a43015fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36(/36) words with w2v vectors\n",
            "Vocab size : 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I had pizza and pasta\"\n",
        "query_vec = infersent.encode(query)[0]\n",
        "print(type(query_vec))\n",
        "print(query_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGV6iEpKwkPU",
        "outputId": "88aed3b6-620b-4e0d-a8b8-98ad3a15c453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[ 0.00746889 -0.14438318 -0.06280056 ... -0.10446591 -0.06125114\n",
            " -0.04683772]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/meetbanthia/Desktop/models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sentences = np.array(sentences)[idx_sort]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for infersent\n",
        "\n",
        "similarity = []\n",
        "for sent in sentences:\n",
        "  sim = cosine(query_vec, infersent.encode([sent])[0])\n",
        "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npsRMhs0zJH0",
        "outputId": "4c52d285-2f92-44c6-f064-82785b3f40e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence =  I ate dinner. ; similarity =  0.41034615\n",
            "Sentence =  We had a three-course meal. ; similarity =  0.20932418\n",
            "Sentence =  Brad came to dinner with us. ; similarity =  0.2429357\n",
            "Sentence =  He loves fish tacos. ; similarity =  0.08098421\n",
            "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.1689744\n",
            "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.23348136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = nlp(\"I had pizza and pasta\")\n",
        "query_vec_s2v = query.vector"
      ],
      "metadata": {
        "id": "MBMI2NXY12l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(query_vec_s2v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNuJJHBV2irZ",
        "outputId": "2a50c179-1412-4354-afa1-ba177a72c0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.939292, -0.993512, -3.98918, -1.4961979, -1.1582799, -2.0926602, -0.78886926, 5.7526803, -2.5939298, 5.3072405, 4.45018, 2.01249, 2.0584562, -0.55372804, 2.823654, -2.590774, -0.576612, -1.684069, -2.72399, -1.129464, 1.95596, 0.84176004, 0.28129, -2.6234062, -0.87262, -2.72368, -2.34796, 0.23158002, -2.27724, -0.03229604, -0.03315394, -3.1126678, 0.28881603, 1.8129282, -0.9115259, -1.895044, -1.9788973, 2.10602, 0.61959606, -0.288446, 0.036623906, 2.190681, -0.6948741, -2.838736, 4.00828, 0.15463401, -1.5779421, -4.1415796, 0.73986614, 3.18483, -0.326464, -0.7224045, 1.100582, -5.79986, 0.72509, -0.81708795, 0.8749441, 0.68788004, 3.9174, 1.899218, 4.4816, -2.2414088, 1.07218, -1.4861, 2.3454597, -1.6128439, -3.2890878, 0.70718604, 0.51750195, 1.93531, -1.2435976, 0.09168501, 0.88116246, 0.47995204, 3.0832124, 1.898798, -4.22452, 1.5561222, -3.22672, -3.433464, -4.64534, 0.89734805, 4.146984, -0.29755798, 2.521834, -1.92814, -1.12564, -2.334956, -0.505514, -2.4918323, -1.351058, 0.031888008, 3.5684218, -4.54718, 1.486994, -0.96676004, 0.88745606, -3.876264, 0.6909979, -0.6179599, 2.4315517, 0.6938001, -0.09031403, 0.76964194, -2.8041599, 2.7805, -4.00637, 1.11524, -1.4832199, -2.554886, 1.1191099, -0.96078205, 2.57364, -1.70554, 2.245902, 1.044434, 1.97492, -2.258248, -0.98641586, -0.12458398, -2.8031, -2.31894, -1.899518, 4.44948, -0.519036, 0.9040081, 1.547232, -4.12844, 0.112877846, -0.723724, -0.430852, -0.05087199, 5.178922, -2.371372, 1.033352, 0.7382666, -3.7481601, -3.8101718, 4.7632055, -2.994598, -1.786096, -0.16769604, -0.90929997, 0.5389, -2.6515996, -1.324788, -0.55578405, -1.062608, -2.3315797, -1.7959, -0.27146006, 0.7846674, -1.7091801, -1.0274599, -1.9869039, 0.7372201, 5.5350003, -0.56557804, -3.111738, -1.27178, 0.10556002, -4.8660803, -0.64555216, 1.187314, -1.1468601, -0.108159974, -1.158168, 1.1242784, -3.311552, 2.85466, 1.934468, -0.10004203, -1.5651001, 2.4062798, 3.1092, -2.1576543, -2.683494, -2.9141622, 1.6350439, 0.16093996, -2.265316, 1.7175999, 3.0704775, 1.68302, -1.523224, 1.6675425, -1.5004021, -0.80685997, 1.5456121, 2.6651661, -2.0706182, 2.33784, -1.445244, 1.5956898, 0.89592, -3.12376, -3.6408582, 0.10766001, -0.769702, 3.6910622, -1.9430221, -0.81235, -0.27486795, 0.42929798, 1.9170601, 1.16935, 1.5827943, -1.215574, 0.664982, 0.0123380665, 1.455266, 1.2846019, 1.1568629, 3.21786, -0.8300371, 0.7882304, -2.917452, 0.24229994, 1.164308, -1.1367226, -0.72158, 1.489836, -1.6177601, 3.148336, 1.5620859, -3.1662, -0.43389, 3.5942402, 1.257938, 0.862834, -1.90268, -2.4795518, 0.58516806, 1.3546541, -0.61556, 1.8597864, -3.8572602, 3.9506278, -1.1428841, -1.39372, -4.2896204, -1.1169399, 1.0108399, 2.849714, 0.29248002, 0.04834194, 0.083927915, -0.21163996, 2.5514839, 4.0794325, -3.2922769, 0.66496205, -6.01794, -0.87477195, -2.2709203, -4.2846994, -1.209332, 0.7270601, -2.2414403, 2.42729, 4.30192, 3.376908, 1.3469719, 2.396418, 3.52259, -1.5315521, 3.9027882, 1.350006, -4.0821733, 3.679786, -0.41656, -0.42924, -3.8564312, -1.60928, -2.8023598, -2.75434, 1.074478, -0.8994659, -0.79377997, 0.38183, -1.2426257, -2.1709542, 1.85296, 2.3292563, 1.960618, -1.356044, -1.9249735, 0.91021997, -3.85064, 0.79175395, 2.9206998, -1.6185601, -1.5873039, -3.6160598, -2.8210201, -0.28041, 1.3313861, 0.9952801, -3.207124, 0.54935825]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for glove sen2vec\n",
        "\n",
        "similarity = []\n",
        "for sent in sentences:\n",
        "  token = nlp(sent)\n",
        "  sim = cosine(token.vector, query_vec_s2v)\n",
        "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1Y2cxTazosB",
        "outputId": "8a4ee55d-0ab8-40cb-f76d-69415ceda20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence =  I ate dinner. ; similarity =  0.7788746\n",
            "Sentence =  We had a three-course meal. ; similarity =  0.67471653\n",
            "Sentence =  Brad came to dinner with us. ; similarity =  0.5343362\n",
            "Sentence =  He loves fish tacos. ; similarity =  0.6137629\n",
            "Sentence =  In the end, we all felt like we ate too much. ; similarity =  0.6539096\n",
            "Sentence =  We all agreed; it was a magnificent evening. ; similarity =  0.6515491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using our infersent model and sen2vec in our dataset**"
      ],
      "metadata": {
        "id": "5aVZ-xhk7ot_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the jgslist and sumlist\n",
        "judgement_sentences = [sentence for sublist in jgslist for sentence in sublist]\n",
        "summary_sentences = [sentence for sublist in sumlist for sentence in sublist]\n",
        "\n",
        "# Build the vocabulary\n",
        "# Build the vocabulary\n",
        "all_sentences = judgement_sentences + summary_sentences\n",
        "infersent.build_vocab(all_sentences, tokenize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCU7FH3K2ohg",
        "outputId": "31f8fe28-5fd1-42ef-abe2-11890f929f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 67948(/144157) words with w2v vectors\n",
            "Vocab size : 67948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see above only 67498/144157 words were trained in a default model.\n",
        "There are many words - legal-domain specific words that were not present in the default embeddings.\n",
        "So we have to do fine tuning to solve this issue.\n",
        "\n",
        "Also even stemming and lemmatization can help reduce your number of OOWs.\n",
        "Stemming and lemmatizing can help reduce the number of out-of-vocabulary words to some extent, but they have different effects on the vocabulary.\n",
        "\n",
        "Stemming is a process of reducing words to their base or root form by removing suffixes. It can help in reducing the number of word forms and treating different variations of a word as the same. However, stemming is a rule-based approach and may not always produce accurate results. It can sometimes result in stemming errors or generate words that are not actual English words.\n",
        "\n",
        "Lemmatizing, on the other hand, reduces words to their base form (lemma) using linguistic rules and morphological analysis. Unlike stemming, lemmatization ensures that the resulting word is a valid word in the language. It provides more accurate results compared to stemming."
      ],
      "metadata": {
        "id": "CMNrCKyklGab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets lemmatize all the words of the dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "jgslist = [[(\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(sent)])) for sent in lst] for lst in jgslist]\n",
        "sumlist = [[(\" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(sent)])) for sent in lst] for lst in sumlist]"
      ],
      "metadata": {
        "id": "ZLbAMge2lFVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the jgslist and sumlist\n",
        "judgement_sentences = [sentence for sublist in jgslist for sentence in sublist]\n",
        "summary_sentences = [sentence for sublist in sumlist for sentence in sublist]\n",
        "\n",
        "# Build the vocabulary\n",
        "# Build the vocabulary\n",
        "all_sentences = judgement_sentences + summary_sentences\n",
        "infersent.build_vocab(all_sentences, tokenize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bmJQudaoMxi",
        "outputId": "2badc609-a2f8-42f1-a6de-84e354a786d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 61668(/137664) words with w2v vectors\n",
            "Vocab size : 61668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still not good"
      ],
      "metadata": {
        "id": "rWQB4FzTw5Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = [\"yo\"]\n",
        "sent = \"Hello how are u\"\n",
        "print([sent])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rNmv9AniVKe",
        "outputId": "df1388ca-5198-40d0-9ff4-b00d8d943039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello how are u']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for a particular sentence is judglist there will be a similarity score\n",
        "# so for sentences in jgslist[i] we have to compare each sentence with sentences in sumlist[i] and all the scores for a sentence in jgslist[i]\n",
        "scores = []\n",
        "foo = [\"legalGO\"]\n",
        "for i in range(len(jgslist)):\n",
        "  temp = []\n",
        "  for j in range(len(jgslist[i])):\n",
        "    #compare jth sentence with all sentences in sumlist[i] and add scores\n",
        "    score = 0\n",
        "    vect1_infersent = infersent.encode([jgslist[i][j]])[0]\n",
        "    vect1_sen2vec = (nlp(jgslist[i][j])).vector\n",
        "    vect_j = np.concatenate((vect1_infersent, vect1_sen2vec))\n",
        "    for sent in sumlist[i]:\n",
        "      vect2_infersent = infersent.encode([sent])[0]\n",
        "      vect2_sen2vec = (nlp(sent)).vector\n",
        "      vect_s = np.concatenate((vect2_infersent, vect2_sen2vec))\n",
        "      score += cosine(vect_j,vect_s)\n",
        "    temp.append(score)\n",
        "  scores.append(temp)"
      ],
      "metadata": {
        "id": "9oyWk7lBw3vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSDdrhNDf9_q",
        "outputId": "b511f7f1-618e-48b7-9e2a-e5f0b980b2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAzH7VEOzb4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}